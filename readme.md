# ğŸ¤– Local RAG - FAQ de MovimentaÃ§Ã£o de Pessoal

Este projeto implementa um **sistema RAG (Retrieval-Augmented Generation)** local, que responde perguntas sobre o documento oficial de **FAQ de MovimentaÃ§Ã£o de Pessoal** do Governo Federal.
A aplicaÃ§Ã£o usa **LangChain**, **FAISS**, **Ollama** e **HuggingFace Embeddings** para buscar informaÃ§Ãµes relevantes no PDF e gerar respostas contextuais com LLM.

---

## ğŸ§© Tecnologias utilizadas

- [LangChain](https://www.langchain.com/) â€” Framework de orquestraÃ§Ã£o de LLMs
- [FAISS](https://faiss.ai/) â€” Banco vetorial para busca semÃ¢ntica
- [HuggingFace Embeddings](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) â€” Modelo de embeddings multilÃ­ngue
- [Ollama](https://ollama.ai/) â€” ExecuÃ§Ã£o local do modelo `llama3.1:8b`
- [PyPDFLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) â€” Leitura e parsing de PDFs

---

## ğŸ“ Estrutura do projeto

```bash
local-rag-faq/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ FAQ - PerguntasFrequentes - MovimentacaoPessoal_v5.1.pdf
â”‚
â”œâ”€â”€ faq_db_faiss/ # Gerado automaticamente apÃ³s o primeiro run
â”‚
â”œâ”€â”€ main.py # Script principal (monta o RAG e executa perguntas)
â”œâ”€â”€ prompts.py # Template de prompt usado pelo agente
â”œâ”€â”€ requirements.txt # DependÃªncias do ambiente
â””â”€â”€ README.md
```

---

## âš™ï¸ InstalaÃ§Ã£o

### 1. Criar ambiente virtual (recomendado com Conda)

```bash
conda create -n LangChainEnv python=3.10
conda activate LangChainEnv
```

### 2. Instalar dependÃªncias

```bash
pip install -r requirements.txt

```

### 3. Instalar e iniciar o Ollama

Baixe e instale o Ollama: https://ollama.ai

Depois, baixe o modelo usado:

ollama pull llama3.1:8b

E deixe o servidor Ollama rodando:

ollama serve

### ğŸ§  Como funciona

Carregamento do documento PDF
â†’ PyPDFLoader extrai o texto.

DivisÃ£o em blocos menores (chunks)
â†’ RecursiveCharacterTextSplitter quebra o texto em segmentos de ~1000 caracteres.

CriaÃ§Ã£o do banco vetorial FAISS
â†’ Cada chunk Ã© convertido em vetor com o modelo paraphrase-multilingual-MiniLM-L12-v2.

Busca semÃ¢ntica (Retriever)
â†’ Dado um prompt do usuÃ¡rio, sÃ£o buscados os k trechos mais relevantes.

GeraÃ§Ã£o da resposta (RAG)
â†’ O contexto recuperado Ã© passado ao LLM llama3.1:8b via Ollama, que gera uma resposta natural e contextualizada.

### ğŸ§© Prompt do agente

O prompt base Ã© definido em prompts.py e controlado por:

```bash
prompt = ChatPromptTemplate.from_template(template)
```

VocÃª pode ajustar o tom ou formato da resposta editando esse template.
