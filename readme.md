# ğŸ¤– Local RAG - FAQ de MovimentaÃ§Ã£o de Pessoal

Este projeto implementa um **sistema RAG (Retrieval-Augmented Generation)** local, que responde perguntas sobre o documento oficial de **FAQ de MovimentaÃ§Ã£o de Pessoal** do Governo Federal.
A aplicaÃ§Ã£o usa **LangChain**, **FAISS**, **Ollama** e **HuggingFace Embeddings** para buscar informaÃ§Ãµes relevantes no PDF e gerar respostas contextuais com LLM.

---

## ğŸ§© Tecnologias utilizadas

- [LangChain](https://www.langchain.com/) â€” Framework de orquestraÃ§Ã£o de LLMs
- [FAISS](https://faiss.ai/) â€” Banco vetorial para busca semÃ¢ntica
- [HuggingFace Embeddings](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) â€” Modelo de embeddings multilÃ­ngue
- [Ollama](https://ollama.ai/) â€” ExecuÃ§Ã£o local do modelo `llama3.1:8b`
- [PyPDFLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) â€” Leitura e parsing de PDFs

---

## ğŸ“ Estrutura do projeto

local-rag-faq/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ FAQ - PerguntasFrequentes - MovimentacaoPessoal_v5.1.pdf
â”‚
â”œâ”€â”€ faq_db_faiss/ # Gerado automaticamente apÃ³s o primeiro run
â”‚
â”œâ”€â”€ main.py # Script principal (monta o RAG e executa perguntas)
â”œâ”€â”€ prompts.py # Template de prompt usado pelo agente
â”œâ”€â”€ requirements.txt # DependÃªncias do ambiente
â””â”€â”€ README.md

---

## âš™ï¸ InstalaÃ§Ã£o

### 1. Criar ambiente virtual (recomendado com Conda)

```bash
conda create -n LangChainEnv python=3.10
conda activate LangChainEnv
2. Instalar dependÃªncias

pip install -r requirements.txt
```

3. Instalar e iniciar o Ollama

Baixe e instale o Ollama: https://ollama.ai

Depois, baixe o modelo usado:

ollama pull llama3.1:8b

E deixe o servidor Ollama rodando:

ollama serve

ğŸš€ ExecuÃ§Ã£o

Com o Ollama rodando, execute o script:

python main.py

Na primeira execuÃ§Ã£o, o sistema:

CarregarÃ¡ o PDF FAQ - PerguntasFrequentes - MovimentacaoPessoal_v5.1.pdf

DividirÃ¡ o texto em chunks

CriarÃ¡ o Ã­ndice vetorial FAISS local (faq_db_faiss/)

ApÃ³s isso, nas execuÃ§Ãµes seguintes, ele carregarÃ¡ o Ã­ndice salvo automaticamente.

ğŸ’¬ Exemplo de saÃ­da
loading vetorial database...
building RAG chain

Question: Como solicitar a movimentaÃ§Ã£o?
--- Contexto Encontrado para a Pergunta: 'solicitar movimentaÃ§Ã£o' ---
Fonte 1:
...

Resposta: Para solicitar a movimentaÃ§Ã£o, vocÃª deve encaminhar informaÃ§Ãµes relativas ao processo Ã  Unidade de GestÃ£o de Pessoas...

ğŸ§  Como funciona

Carregamento do documento PDF
â†’ PyPDFLoader extrai o texto.

DivisÃ£o em blocos menores (chunks)
â†’ RecursiveCharacterTextSplitter quebra o texto em segmentos de ~1000 caracteres.

CriaÃ§Ã£o do banco vetorial FAISS
â†’ Cada chunk Ã© convertido em vetor com o modelo paraphrase-multilingual-MiniLM-L12-v2.

Busca semÃ¢ntica (Retriever)
â†’ Dado um prompt do usuÃ¡rio, sÃ£o buscados os k trechos mais relevantes.

GeraÃ§Ã£o da resposta (RAG)
â†’ O contexto recuperado Ã© passado ao LLM llama3.1:8b via Ollama, que gera uma resposta natural e contextualizada.

ğŸ§© Prompt do agente

O prompt base Ã© definido em prompts.py e controlado por:

prompt = ChatPromptTemplate.from_template(template)

VocÃª pode ajustar o tom ou formato da resposta editando esse template.
